"""OutputAgent - Valida e modera respostas usando LLM."""

from typing import Dict, Any
import os
from agents.base_agent import BaseAgent
from config import GEMINI_MODEL, GEMINI_API_KEY, GOOGLE_CLOUD_PROJECT
from llm_client import create_llm_client


class OutputAgent(BaseAgent):
    """
    Agente respons√°vel por validar e moderar respostas finais usando LLM.
    
    Combina as funcionalidades de ValidadorAgent e ModeratorAgent em um √∫nico agente inteligente.
    """
    
    def __init__(self):
        super().__init__("OutputAgent")
        
        # Configura LLM (Gemini API ou Vertex AI) para valida√ß√£o inteligente
        # O LLMClient detecta automaticamente qual usar baseado na chave
        if GEMINI_API_KEY:
            try:
                self.llm_client = create_llm_client(
                    api_key=GEMINI_API_KEY,
                    model_name=GEMINI_MODEL,
                    project_id=GOOGLE_CLOUD_PROJECT
                )
                self.model = self.llm_client.model  # Compatibilidade com c√≥digo existente
                self.log(f"LLM configurado ({self.llm_client.client_type}, {GEMINI_MODEL}) para valida√ß√£o inteligente")
            except Exception as e:
                self.llm_client = None
                self.model = None
                self.log(f"Erro ao configurar LLM: {e}. Usando fallback", "WARNING")
        else:
            self.llm_client = None
            self.model = None
            self.log("LLM n√£o configurado - usando fallback", "WARNING")
    
    def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Valida e modera resposta usando LLM Gemini.
        
        O LLM verifica:
        - Resposta est√° completa e coerente?
        - Tom √© amig√°vel e profissional?
        - Tem informa√ß√µes necess√°rias?
        - Precisa adicionar/ajustar emojis?
        - Formata√ß√£o Markdown est√° correta?
        
        Args:
            data: {
                "response": str (resposta a validar),
                "intent": str (inten√ß√£o detectada),
                "user_phone": str (opcional)
            }
            
        Returns:
            {
                "response": str (resposta melhorada),
                "valid": bool,
                "error": str (se houver)
            }
        """
        response = data.get("response", "")
        intent = data.get("intent", "")
        
        # Valida√ß√£o b√°sica de seguran√ßa (n√£o usa LLM - r√°pido)
        if not response or len(response.strip()) == 0:
            self.log("Resposta vazia detectada", "ERROR")
            return {
                **data,
                "response": "Desculpe, n√£o consegui gerar uma resposta.",
                "valid": False,
                "error": "Resposta vazia"
            }
        
        # Trunca se muito longo (aumentado para permitir respostas mais detalhadas)
        if len(response) > 8000:
            response = response[:8000]
            self.log("Resposta truncada para 8000 caracteres", "WARNING")
        
        # Se n√£o tem LLM, retorna com valida√ß√£o b√°sica
        if not self.llm_client or not self.llm_client.model:
            self.log("Usando fallback sem LLM")
            return {
                **data,
                "response": response,
                "valid": True,
                "error": None
            }
        
        # Usa LLM para validar e melhorar APENAS se necess√°rio
        # Verifica se a resposta j√° est√° bem formatada antes de modificar
        try:
            # Verifica se resposta j√° tem formata√ß√£o adequada
            has_emojis = any(emoji in response for emoji in ["‚úÖ", "‚ùå", "‚ö†Ô∏è", "üí∞", "üìä", "üéâ", "üìù", "üìã", "üî¥", "üü°", "üü¢"])
            has_markdown = "*" in response or "_" in response or "`" in response
            is_well_formatted = has_emojis and has_markdown and len(response) > 20
            
            # Se j√° est√° bem formatada, retorna sem modificar
            if is_well_formatted:
                self.log("Resposta j√° bem formatada, retornando sem modifica√ß√£o")
                return {
                    **data,
                    "response": response,
                    "valid": True,
                    "error": None
                }
            
            # S√≥ modifica se realmente necess√°rio
            prompt = f"""Voc√™ √© um validador e moderador de respostas de chatbot financeiro.

**Contexto:** {intent}

**Resposta a validar:**
"{response}"

**Sua tarefa:**
1. Se a resposta j√° est√° boa e formatada, retorne ela EXATAMENTE como est√°, sem mudan√ßas
2. Se precisa melhorar, fa√ßa APENAS ajustes m√≠nimos:
   - Adicione emojis apropriados se n√£o tiver (‚úÖ ‚ùå ‚ö†Ô∏è üí∞ üìä üéâ üìù)
   - Corrija erros de formata√ß√£o Markdown APENAS se houver problemas
3. NUNCA altere n√∫meros, valores ou informa√ß√µes importantes
4. Mantenha o portugu√™s brasileiro natural

**IMPORTANTE:**
- Se a resposta j√° est√° perfeita, retorne ela EXATAMENTE como est√°
- N√£o invente informa√ß√µes novas
- Mantenha n√∫meros e valores exatos
- Fa√ßa mudan√ßas M√çNIMAS apenas se necess√°rio
- Retorne APENAS a resposta melhorada, SEM explica√ß√µes ou coment√°rios

**Resposta melhorada:**"""
            
            # Usa LLMClient que funciona com ambas as APIs
            result = self.llm_client.generate_content(prompt)
            improved_response = result.text.strip()
            
            # Se a resposta melhorada estiver vazia, usa a original
            if not improved_response:
                self.log("LLM retornou resposta vazia, usando original", "WARNING")
                improved_response = response
            
            self.log("Resposta validada e melhorada com LLM (apenas se necess√°rio)")
            
            return {
                **data,
                "response": improved_response,
                "valid": True,
                "error": None
            }
        
        except Exception as e:
            self.log(f"Erro no LLM de valida√ß√£o: {e}", "ERROR")
            # Fallback: retorna original sem modifica√ß√£o
            # Isso garante que o usu√°rio sempre recebe uma resposta
            return {
                **data,
                "response": response if response else "Desculpe, n√£o consegui processar sua solicita√ß√£o.",
                "valid": True,
                "error": f"Erro no LLM: {str(e)}"
            }

